{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['Cow', 'Dolhpin', 'Chicken', 'Seal', 'Penguin', 'Bat']\n",
    "\n",
    "hypotheses = [\n",
    "    [1,1,0,1,0,1], # Viviparous (definitionally all mammals in set)\n",
    "    [0,0,1,0,1,0], # Oviparous\n",
    "    [0,0,1,0,1,1], # Winged\n",
    "    [0,0,1,0,1,0], # Winged non-flying\n",
    "    [1,0,1,0,1,1], # Separate 2 hind limbs\n",
    "    [0,1,0,1,1,0], # Marine habitat\n",
    "    [1,0,1,0,0,0], # Commonly found on farms\n",
    "    [0,0,0,1,1,0], # Commonly found in cold areas\n",
    "    [0,1,0,1,1,0], # Carnivorous\n",
    "    [1,0,0,0,0,0], # Herbivorous\n",
    "    [0,0,1,0,0,1], # Omnivorous\n",
    "    [1,1,1,1,1,1], # Animals\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 2, 4, 3, 2, 2, 3, 1, 2, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hypotheses = np.array(hypotheses)\n",
    "display(hypotheses.shape)\n",
    "\n",
    "hypotheses_size = hypotheses.sum(axis=1)\n",
    "display(hypotheses_size)\n",
    "\n",
    "hypotheses_names = ['Viviparous', 'Oviparous', 'Winged', 'Winged non-flying', 'Separate 2 hind limbs', 'Marine habitat', 'Commonly found on farms', 'Commonly found in cold areas', 'Carnivorous', 'Herbivorous', 'Omnivorous', 'Animals']\n",
    "hypotheses_named = {name: hypothesis for name, hypothesis in zip(hypotheses_names, hypotheses)}\n",
    "\n",
    "# display(hypotheses_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## Prior distribution\n",
    "\n",
    "A prior distribution I think may heuristically address the concern described below is to have the prior probability of each hypothesis $h_i$ be proportional to the inverse of the sum of the L0 norm distances (themselves normalized) to all hypothesis (including to $h_i$ itself).\n",
    "\n",
    "## Motivation and Justification\n",
    "\n",
    "It is plausible to expect that, when coming up with hypotheses, I may have created more hypotheses involving certain subsets of animals simply because they came to mind more readily. In the extreme, I could include many copies of nearly-identical hypotheses despite the fact that they may not be representative of all plausible hypotheses.\n",
    "\n",
    "The motivation for this approach is that a hypothesis begins with a prior likelihood of 1 divided by a denominator. Because a hypothesis has a normalized L0 similarity of 1 to itself, that denominator has at the very least a value of 1. The normalized similarity of each other hypothesis is added to the denominator. Hence, the more similarities which exist relative to other hypotheses, the smaller the prior likelihood will be.\n",
    "\n",
    "Consider example hypotheses $h_1, h_2, h_3$, where $h_1 = h_2$ and $h_3 = \\neg h_1$. Under this prior distribution, since $h_1 = h_2$, both should have a prior prior equal to $1/(1+1+0) = 1/2$. $h_3$ will have a prior equal to $1/(1+0+0) = 1$.\n",
    "\n",
    "Note: Some hypotheses which are defined by membership, others functionally by non-membership. The L0 norm does not identify that some hypotheses may be quite similar, just with their bits flipped. One way to address this such that our distribution still achieves similar results would be to not only compute L0 scores with all other hypotheses, but with the negations of those hypotheses as well. However, we leave that for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11, 0.07, 0.07, 0.07, 0.08, 0.09, 0.08, 0.08, 0.09, 0.09, 0.08,\n",
       "       0.09])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarities = np.empty(shape=(len(hypotheses), len(hypotheses)))\n",
    "for i, h_i in enumerate(hypotheses):\n",
    "    sim = [len(animals) - np.linalg.norm(h_i - h_o, ord=0) for o, h_o in enumerate(hypotheses)]\n",
    "    similarities[i] = np.array(sim)\n",
    "# display(similarities)\n",
    "\n",
    "similarities_norm = similarities / len(animals) # max similarity should = 1, least smilarity = 0\n",
    "# display(similarities_norm.round(decimals=2))\n",
    "\n",
    "similarities_norm_sum = similarities_norm.sum(axis=1)\n",
    "# display(similarities_norm_sum)\n",
    "\n",
    "h_likelihoods = 1 / similarities_norm_sum\n",
    "# display(h_likelihoods.round(decimals=2))\n",
    "\n",
    "h_probabilities_prior = h_likelihoods / h_likelihoods.sum()\n",
    "display(h_probabilities_prior.round(decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_posterior_h_given_x(h, x, f_prob_x_given_h):\n",
    "    # h is the index of a particular hypothesis\n",
    "    # x is an index of a particular animal\n",
    "    # f_prob_x_given_h is a function that depends on weak/strong sampling\n",
    "    numerator = h_probabilities_prior[h] * f_prob_x_given_h(x,h)\n",
    "    denominator = [h_probabilities_prior[h_prime] * f_prob_x_given_h(x,h_prime) for h_prime in range(len(hypotheses))]\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak sampling\n",
    "\n",
    "def prob_x_given_h_weak(x, h):\n",
    "    return 1 if h[x] == 1 else 0\n",
    "\n",
    "def prob_posterior_h_given_x_weak(h, x):\n",
    "    return prob_posterior_h_given_x(h, x, prob_x_given_h_weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strong sampling\n",
    "\n",
    "def prob_x_given_h_strong(x, h):\n",
    "    return 1/hypotheses_size[h] if h[x] == 1 else 0\n",
    "\n",
    "def prob_posterior_h_given_x_strong(h, x):\n",
    "    return prob_posterior_h_given_x(h, x, prob_x_given_h_strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
