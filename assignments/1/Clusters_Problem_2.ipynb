{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian 2-Clusters\n",
    "In this problem, we will investigate how to make categorization decisions for two categories, where each is defined as a Gaussian distribution. First, you will derive the probability of an item being in one category or another. Then, we will explore how the variances and prior probability of each category affect the posterior and predictive distributions. \n",
    "\n",
    "For this problem, we will assume that data are generated by first picking which of two categories $c=1,2$ it belongs to (according to their prior probability) and then generating the datum according to the corresponding category's likelihood. This results in the following generative process:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "c_n | \\theta \\sim {\\rm Bernoulli}(\\theta)   & \\qquad \\qquad \\qquad & x_n | \\mu_{c(n)}, \\sigma_{c(n)}^2 \\overset{iid}{\\sim} {\\rm N}(\\mu_{c(n)}, \\sigma_{c(n)}^2)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Note that $c(n)$ is the same as $c_n$. I use parentheses rather than a subscript to avoid having something that is \n",
    "``double subscripted.''. In generative process notation, $c_n | \\theta \\sim {\\rm Bernoulli}(\\theta)$ means that $c_n$, the category for data point $n$, is a Bernoulli random variable with parameter $\\theta$. This means $c_n$ will be $1$ with probability $\\theta$ (and so with probability $1-\\theta$, $c_n=2$). So, the prior probability of category 1 is $\\theta$ ($P(c_n=1)=\\theta$). For all of Problem 2, assume $\\mu_1=-1$ and $\\mu_2=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Derivation: Categorization\n",
    "\n",
    " Using Bayes' rule, derive the probability of a single datum being in category 1: $P(c_1=1|x_1)$.\\footnote{If you are wondering why $P$ is uppercase here and lowercase in other cases, it has to do with whether it is a probability mass (uppercase) or density (lowercase) function. You do not have to worry about this distinction (I mess it up sometimes too!), but as a general rule, you should use the uppercase $P(\\cdot)$ when it is a discrete random variable and the lowercase $p(\\cdot)$ when it is a continuous random variable.} You can assume that the values of $\\mu_1, \\mu_2, \\sigma^2_1,$ and $\\sigma^2_2$ are given parameters (I didn't put them to the right of the $|$ to save space). Show your work (if you do not know how to create equations on a computer, you can scan your handwritten derivation and include it as an image into your document). \n",
    " \n",
    "As the next problem depends on this answer, I will give you what your derivation should end up with. It is\n",
    " \n",
    " $\n",
    " \\begin{equation*}\n",
    "P(c=1|x_1) = \\frac{\\theta {\\rm N}(x_1; \\mu_1, \\sigma_1^2)}{\\theta {\\rm N}(x_1; \\mu_1, \\sigma_1^2) + (1-\\theta){\\rm N}(x_1; \\mu_2, \\sigma_2^2)}\n",
    "\\end{equation*}\n",
    " $\n",
    " \n",
    "where $N(x;\\mu,\\sigma^2)$ is the probability density of $x$ from a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Categorization\n",
    "\n",
    "Calculate and plot the probability of being in category 1 (so, the x-axis is the $x_1$ value and the y-axis is $P(c_1=1|x_1)$) for $\\theta = 0.5$ and for $\\theta = 0.75$ with $\\sigma_1^2=\\sigma_2^2=1$ (remember $\\mu_1 = -1$ and $\\mu_2=1$). Next, calculate and plot the probability of being in category 1 for $\\theta = 0.5$ when $\\sigma_1^2=0.5$ and $\\sigma_2^2=2$. Do this again, but for $\\theta=0.75$. Please make sure all of your plots show all of the interesting behavior (so make sure the range of your x- and y-axes are appropriate). Describe the effect of changing the prior and the variance on categorization decisions. What is the effect of varying them? Do they have the same effect? Why do they or why do they not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Derivation --- Prediction \n",
    "\n",
    "Using Bayes' rule and the {\\em Law of Total Probability}, derive the probability of a data point $p(x)$ according to this model (note, this is without any given data). As the next problem depends on this answer, I will give you what your derivation should end up with. It is\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "p(x_1) = \\theta {\\rm N}(x_1; \\mu_1, \\sigma_1^2) + (1-\\theta){\\rm N}(x_1; \\mu_2, \\sigma_2^2)\n",
    "\\end{equation*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Prediction\n",
    "\n",
    "As before, plot $p(x_1)$ for $\\theta = 0.5$, and then for $\\theta =  0.75$ with $\\sigma_1^2=\\sigma_2^2=1$. Next, calculate and plot $P(x_1)$ for $\\theta = 0.5$ when $\\sigma_1^2=0.5$ and $\\sigma_2^2=2$. Do this again, but for $\\theta=0.75$. How does the prior and variance of the likelihood affect $p(x_1)$?  Please make sure all of your plots show all of the interesting behavior (so make sure the range of your x- and y-axes are appropriate). Describe the effect of changing the prior and the variance on $p(x_1)$. What is the effect of varying them? Do they have the same effect? Why do they or why do they not? \n",
    "\n",
    "Note that $p(x_1)$ is sometimes called the *marginal data distribution* and this type of model is called a *mixture model* because it composes a new probability distribution by``mixing'' two (or more) distributions together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
