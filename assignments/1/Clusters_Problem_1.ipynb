{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6_4sYWzgjyx"
   },
   "source": [
    "## Gaussians\n",
    "\n",
    "In class, we derived (or will derive) the posterior and predictive distributions for a data point generated from a Gaussian-Gaussian model: having a Gaussian likelihood with unknown mean and known variance, and with a Gaussian prior on the mean of the likelihood with known mean and known variance. This model can be written in generative process notation\\footnote{I try to use the following convention for variables: scalar variables are lowercase and italics (e.g., $x$), indices tend to be $n$, $t$, $c$ or $k$, the largest index is uppercase (e.g., $N$), vector variables are bold and lowercase (e.g., ${\\bf x}$), and matrix variables are bold and uppercase (e.g., ${\\bf X}$).} as:\n",
    "\n",
    "$\\mu \\sim {\\rm N}(\\mu_0, \\sigma^2_0)$\n",
    "$x_1,\\dotsc, x_N | \\mu, \\sigma_x^2 \\overset{iid}{\\sim} {\\rm N}(\\mu, \\sigma^2_x)$\n",
    "\n",
    "Remember that $iid$ means {\\em independent} and {\\em identically distributed} and the generative process notation $x | \\mu, \\sigma_x^2 \\sim {\\rm N}(\\mu, \\sigma_x^2)$ means that given the values of parameters $\\mu$ and $\\sigma_x^2$, $x$ is normally distributed with mean $\\mu$ and variance $\\sigma_x^2$. So, this means\n",
    "$p(x|\\mu,\\sigma_x^2) = \\frac{1}{\\sigma_x \\sqrt{2 \\pi}} e^{-\\frac{1}{2 \\sigma_x^2}\\left(x-\\mu \\right)^2}$\n",
    "\n",
    "Note that it is traditional to use a zero subscript for the parameters for a prior distribution. For this model, there is a closed form solution for the posterior probability, $p(\\mu | x_1, \\dotsc, x_N)$, and predictive probability, $p(x_{N+1} | x_1,\\dotsc,x_N)$. Remember that you can always look up these special models whose posterior distribution is the same form as the prior distribution on Wikipedia [Conjugate Prior](https://en.wikipedia.org/wiki/Conjugate_prior). They are\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "{\\rm Posterior:} & &  \\mu & | x_1,\\dotsc, x_N \\sim {\\rm N} \\left( \\frac{\\mu_0 \\sigma_0^{-2} + \\sigma_x^{-2} \\sum_{n=1}^N{x_n}   } {\\sigma_0^{-2} + N \\sigma_x^{-2} }, \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1}  \\right)  \\\\\n",
    "{\\rm Prediction} & & x_{N+1} & | x_1, \\dotsc, x_N \\sim {\\rm N} \\left( \\frac{ \\mu_0 \\sigma_0^{-2} + \\sigma_x^{-2} \\sum_{n=1}^N{x_n}  }{\\sigma_0^{-2} + N \\sigma_x^{-2} },  \\left[ \\sigma_0^{-2} + N \\sigma_x^{-2} \\right]^{-1} + \\sigma_x^2 \\right)\n",
    "\\end{align*}\n",
    "$\n",
    "So, the predictive distribution has the same mean as the posterior distribution, but it has larger variance (it is $\\sigma_x^2$ larger). For this problem, use $\\mu_0=0$ and $\\sigma_0^2 = 1$.  In this problem, we will explore how the number of data points and variance of the likelihood affect the posterior and predictive distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GoWENrDAgjyy",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4692ba65783cc82f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1. (a) Prior\n",
    "To provide a baseline, turn in a plot of the prior distribution. Please make sure your plot captures the ``interesting'' part of the distribution (i.e., the two extrema of the x-axis are the tails and the width and maximum of the bell are clearly visible). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwfnS1ztgjyz",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e94ea5daaba0d252",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#fill me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWiAv-6mgs0W"
   },
   "source": [
    "### 1. (b) One Datum update\n",
    "Calculate and plot the posterior and predictive distributions after observing $x_1=2$ for $\\sigma_x^2 = 0.25$ and $\\sigma_x^2=4$ (that is 4 different distributions: the posterior and predictive for $\\sigma_x^2=0.25$ and the posterior and predictive for $\\sigma_x^2=4$). How does changing the variance of the likelihood affect the distributions? Are there any differences? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "dYDwX_zmhLPm",
    "outputId": "f241b967-c527-4af9-fda0-f6ef29fab651"
   },
   "outputs": [],
   "source": [
    "#fill me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jdWz5qYow_u"
   },
   "source": [
    "### 1. (c) Multiple data update\n",
    "Calculate and plot the posterior and predictive distributions given $(x_1,\\dotsc, x_5) = (2.1, 2.5, 1.4, 2.2, 1.8)$ for $\\sigma_x^2 = 0.25$ and $\\sigma_x^2 = 4$. How does this  compare to the previous example? Note that the average of the data points is 2, and so both contribute the same average value. For cases that differ, why do they differ then? For those that do not, why don't they differ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "JYOEuzB5o8gx",
    "outputId": "3ded2d78-ef74-4b6a-f5a5-d21df62c4762"
   },
   "outputs": [],
   "source": [
    "fill me"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "Problem 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
