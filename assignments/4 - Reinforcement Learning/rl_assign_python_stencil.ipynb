{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpN28_Ub4kuF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Mostly written by Mark Ho with edits from Joe Austerweil\n",
        "so students don't have the answers.\n",
        "\n",
        "Ported to python 3 by Guangfei Zhu\n",
        "Dependencies:\n",
        "numpy, itertools, copy, random\n",
        "\"\"\"\n",
        "from itertools import product\n",
        "import copy, random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, Arrow, Circle\n",
        "from matplotlib.path import Path\n",
        "import matplotlib.patheffects as path_effects\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def toStateActionNextstateRF(rf, tf):\n",
        "    rf_value = list(rf.values())\n",
        "    if type(rf_value[0]) == dict:\n",
        "        is_stateRF = False\n",
        "    else:\n",
        "        is_stateRF = True\n",
        "\n",
        "    temp_rf = {}\n",
        "    for s, a_ns_prob in tf.items():\n",
        "        temp_rf[s] = {}\n",
        "        for a, ns_prob in a_ns_prob.items():\n",
        "            temp_rf[s][a] = {}\n",
        "            for ns, prob in ns_prob.items():\n",
        "                if is_stateRF:\n",
        "                    temp_rf[s][a][ns] = rf[ns]\n",
        "                else:\n",
        "                    temp_rf[s][a][ns] = rf[s][a]\n",
        "    return temp_rf\n",
        "\n",
        "def calc_stochastic_policy(action_vals, rand_choose=0.0):\n",
        "    s_policy = {}\n",
        "    if rand_choose == 0.0:\n",
        "        for s, a_q in action_vals.iteritems():\n",
        "            acts, qs = zip(*a_q.items())\n",
        "            max_q = max(qs)\n",
        "            max_acts = [acts[i] for i, qv in enumerate(qs) if qv == max_q]\n",
        "            probs = [1/len(max_acts) for _ in max_acts]\n",
        "            s_policy[s] = dict(zip(max_acts, probs))\n",
        "    else:\n",
        "        for s, a_q in action_vals.items():\n",
        "            acts, qs = zip(*a_q.items())\n",
        "            max_q = max(qs)\n",
        "            max_acts = [acts[i] for i, qv in enumerate(qs) if qv == max_q]\n",
        "            nonmax_prob = rand_choose/len(acts)\n",
        "            max_prob = (1-rand_choose)/len(max_acts) + nonmax_prob\n",
        "            probs = [max_prob if a in max_acts else nonmax_prob for a in acts]\n",
        "            s_policy[s] = dict(zip(acts, probs))\n",
        "    return s_policy\n",
        "\n",
        "def deterministicVI(rf, tf, init_state=None, max_iterations=30, delta=0.001,\n",
        "                    gamma=0.99, fixed_action_order=True, print_info=True):\n",
        "    \"\"\"Finds an optimal deterministic policy given a\n",
        "            reward function: {s: {a: {ns : r } }, and\n",
        "            transition function: {s : {a : {ns : prob}}}\n",
        "    \"\"\"\n",
        "    rf_value = list(rf.values())\n",
        "    if type(rf_value[0]) != dict:\n",
        "        rf = toStateActionNextstateRF(rf, tf)\n",
        "    elif type(list(rf_value[0].values())[0]) != dict:\n",
        "        rf = toStateActionNextstateRF(rf, tf)\n",
        "\n",
        "    states = [s for s in tf.keys()]\n",
        "    if fixed_action_order:\n",
        "        #always consistent ordering of actions\n",
        "        state_actions = dict([(s, sorted(a.keys())) for s, a in tf.items()])\n",
        "    else:\n",
        "        #random but consistent ordering of actions\n",
        "        state_actions = {}\n",
        "        for s, a in tf.items():\n",
        "            state_actions[s] = sorted(a.keys(), key=lambda _: random())\n",
        "    vf = dict([(s, 0.0) for s in states])\n",
        "    op = {}\n",
        "    action_vals = {}\n",
        "    for s, actions in state_actions.items():\n",
        "        op[s] = actions[random.randint(0, len(actions)-1)]\n",
        "        action_vals[s] = dict(zip(actions, [0.0]*len(actions)))\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        change = 0\n",
        "        vf_temp = {}\n",
        "        for state, actions in state_actions.items():\n",
        "            max_action = actions[0]\n",
        "            max_action_val = -np.inf\n",
        "            for action in actions:\n",
        "                #calculate expected utility of each action\n",
        "                action_vals[state][action] = 0\n",
        "                for ns, prob in tf[state][action].items():\n",
        "                    update = prob*(rf[state][action][ns] + gamma*vf[ns])\n",
        "                    action_vals[state][action] += update\n",
        "                if max_action_val < action_vals[state][action]:\n",
        "                    max_action = action\n",
        "                    max_action_val = action_vals[state][action]\n",
        "            vf_temp[state] = max_action_val\n",
        "            op[state] = max_action\n",
        "            change = max(change, abs(vf_temp[state]-vf[state]))\n",
        "        vf = vf_temp\n",
        "        if print_info:\n",
        "            print('iteration: %d   change: %.2f' % (i, change))\n",
        "        if change < delta:\n",
        "            break\n",
        "    return op, vf, action_vals\n",
        "\n",
        "def calc_softmax_policy(action_vals, temp=1):\n",
        "    soft_max_policy = {}\n",
        "    for s, a_q in action_vals.items():\n",
        "        a_q = a_q.items()\n",
        "        sm = np.exp([(q/temp) for a, q in a_q])\n",
        "        sm = list(sm/np.sum(sm))\n",
        "        soft_max_policy[s] = dict(zip([a for a, q in a_q], sm))\n",
        "    return soft_max_policy\n",
        "\n",
        "def sample_prob_dict(pdict):\n",
        "    outs, p_s = zip(*pdict.items())\n",
        "    out_i = np.random.choice(range(len(outs)), p=p_s)\n",
        "    return outs[out_i]\n",
        "\n",
        "class Qlearning(object):\n",
        "    def __init__(self, mdp,\n",
        "                 decision_rule='egreedy',\n",
        "                 egreedy_epsilon=.2,\n",
        "                 softmax_temp=1,\n",
        "                 discount_rate=None,\n",
        "                 learning_rate=.25,\n",
        "                 eligibility_trace_method='replacement',\n",
        "                 eligiblity_trace_decay=0,\n",
        "                 initial_qvalue=0,\n",
        "                 init_state=None\n",
        "                 ):\n",
        "\n",
        "        self.mdp = mdp\n",
        "        self.decision_rule = decision_rule\n",
        "        self.egreedy_epsilon = egreedy_epsilon\n",
        "        self.softmax_temp = softmax_temp\n",
        "        self.learning_rate = learning_rate\n",
        "        self.eligibility_trace_method = eligibility_trace_method\n",
        "        self.eligiblity_trace_decay = eligiblity_trace_decay\n",
        "        self.initial_qvalue = initial_qvalue\n",
        "        #self.r = self.mdp.gen_reward_dict()\n",
        "\n",
        "        self.qvalues = {}\n",
        "        self.eligibility_traces = {}\n",
        "\n",
        "        if init_state is None:\n",
        "            init_state = mdp.get_init_state()\n",
        "        self.init_state = init_state\n",
        "        if discount_rate is None:\n",
        "            discount_rate = mdp.discount_rate\n",
        "        self.discount_rate = discount_rate\n",
        "\n",
        "\n",
        "    def get_action(self, s):\n",
        "        #initialize qvalues if not in dictionary\n",
        "        if s not in self.qvalues:\n",
        "            self.qvalues[s] = {}\n",
        "            for a in self.mdp.available_actions(s):\n",
        "                self.qvalues[s][a] = self.initial_qvalue\n",
        "\n",
        "        #special case of a single action\n",
        "        if len(self.qvalues[s]) == 1:\n",
        "            s_keys = list(self.qvalues[s].keys())\n",
        "            return s_keys[0]\n",
        "#            return self.qvalues[s].keys()[0]\n",
        "\n",
        "        #select a decision rule\n",
        "        qs = list(self.qvalues[s].items())\n",
        "        if self.decision_rule == 'egreedy':\n",
        "            #print(f\"qs: {qs}\")\n",
        "           # print('in egreedy')\n",
        "            if np.random.random() > self.egreedy_epsilon:\n",
        "                max_q = max(qs, key=lambda aq: aq[1])[1]\n",
        "                maxactions = [a for a, q in qs if q == max_q]\n",
        "                max_i = np.random.choice(range(len(maxactions)))\n",
        "                return maxactions[max_i]\n",
        "            else:\n",
        "                ai = np.random.choice(range(len(qs)))\n",
        "                return qs[ai][0]\n",
        "        elif self.decision_rule == 'softmax':\n",
        "            qvals = np.array([q for a, q in qs])\n",
        "            qvals = np.exp(qvals/self.softmax_temp)\n",
        "            probs = qvals/np.sum(qvals)\n",
        "            actions = [a for a, q in qs]\n",
        "            return actions[np.random.choice(range(len(qs)), p=probs)]\n",
        "\n",
        "\n",
        "    def process(self, s, a, ns, r):\n",
        "        #update dictionaries as needed\n",
        "        if ns not in self.qvalues:\n",
        "            self.qvalues[ns] = {}\n",
        "            for a_ in self.mdp.available_actions(ns):\n",
        "                self.qvalues[ns][a_] = self.initial_qvalue\n",
        "\n",
        "        if s not in self.qvalues:\n",
        "            self.qvalues[s] = {}\n",
        "            for a_ in self.mdp.available_actions(s):\n",
        "                self.qvalues[s][a_] = self.initial_qvalue\n",
        "\n",
        "\n",
        "        ns_action_values = self.qvalues[ns]\n",
        "        max_q_ns = max(ns_action_values.values())\n",
        "\n",
        "        #calculate prediction error (term in parentheses of equation 1)\n",
        "        # note: r = reward for this instance, s = state, and ns = next state\n",
        "        # max_q_ns is the maximum q-value achieveable from next state.\n",
        "        # self.discount_rate is the discount rate\n",
        "        # self.q-values[s][a] is the current q-value for taking action a in state s\n",
        "\n",
        "       #FILL IN:\n",
        "\n",
        "       #pred_error =  #fill in\n",
        "\n",
        "        # can uncomment for debugging information\n",
        "        # print(f\"cur_state: {s};       action: {a}      next_state: {ns}\")\n",
        "        # print(f\"cur action values: {ns_action_values}\")\n",
        "        # print(f\"current reward {r}\")\n",
        "        # print(f\"current q_table: {self.qvalues[s][a]}\")\n",
        "\n",
        "        q_update = self.learning_rate * pred_error\n",
        "        self.qvalues[s][a] += q_update\n",
        "\n",
        "\n",
        "    def reset_eligibility_traces(self):\n",
        "        self.eligibility_traces = {}\n",
        "\n",
        "    def run(self,\n",
        "            episodes=100, max_steps=100,\n",
        "            init_state=None):\n",
        "        if init_state is None:\n",
        "            init_state = self.init_state\n",
        "\n",
        "\n",
        "        run_data = []\n",
        "\n",
        "        for e in range(episodes):\n",
        "            s = init_state\n",
        "            for t in range(max_steps):\n",
        "                a = self.get_action(s)\n",
        "                ns = self.mdp.transition(s=s, a=a)\n",
        "                r = self.mdp.reward(s=s, a=a, ns=ns)\n",
        "                # print({\n",
        "                #     'episode': e, 'timestep': t,\n",
        "                #     's': s, 'a': a, 'ns': ns, 'r': r\n",
        "                # })\n",
        "                run_data.append({\n",
        "                    'episode': e, 'timestep': t,\n",
        "                    's': s, 'a': a, 'ns': ns, 'r': r\n",
        "                })\n",
        "                self.process(s, a, ns, r)\n",
        "                s = ns\n",
        "                if self.mdp.is_terminal(ns):\n",
        "                    break\n",
        "        return run_data\n",
        "\n",
        "    def get_softmax_policy(self, temp=1):\n",
        "        return calc_softmax_policy(self.qvalues, temp)\n",
        "\n",
        "    def get_egreedy_policy(self, rand_choose=.2):\n",
        "        return calc_stochastic_policy(self.qvalues, rand_choose)\n",
        "\n",
        "class RewardFunction(object):\n",
        "    \"\"\"\n",
        "    TODO: implement state-action and state-action-nextstate feature rfs\n",
        "\n",
        "    state_features : dict with mapping from states to lists of features\n",
        "    feature_rewards : dict with mapping from features to reward values\n",
        "\n",
        "    Reward is simply the sum of all features (for now). This implementation\n",
        "    represents reward functions based on either states; states and actions, or\n",
        "    states, actions, and nextstates. Orthogonally, it can represent them\n",
        "    in tabular form, or as sums of features (over either states, states/actions,\n",
        "    or states/actions/nextstates.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 state_features=None,\n",
        "                 state_rewards=None, #this is a deprecated argument\n",
        "                 reward_dict=None,\n",
        "                 feature_rewards=None,\n",
        "                 default_reward=0,\n",
        "                 terminal_states=None,\n",
        "                 terminal_state_reward=0,\n",
        "                 step_cost=0,\n",
        "                 rmax=None,\n",
        "                 cache_rewards=True):\n",
        "        if terminal_states is None:\n",
        "            terminal_states = [(-1, -1), (-2, -2)]\n",
        "        if state_rewards is not None:\n",
        "            reward_dict = state_rewards\n",
        "\n",
        "        self.terminal_states = tuple(sorted(terminal_states))\n",
        "        self.terminal_state_reward = terminal_state_reward\n",
        "        self.default_reward = default_reward\n",
        "#        print('self.terminal_states:',self.terminal_states)\n",
        "#        print('feature_reward:',feature_rewards)\n",
        "\n",
        "        if (state_features is not None) and (feature_rewards is not None):\n",
        "            self.state_features = state_features\n",
        "            self.feature_rewards = feature_rewards\n",
        "            self.type = 'state_feature_based'\n",
        "        elif reward_dict is not None:\n",
        "            self.reward_dict = copy.deepcopy(reward_dict)\n",
        "            reward_dict_value = list(reward_dict.values())\n",
        "#            print(reward_dict_value[0])\n",
        "            if type(reward_dict_value[0]) is dict:\n",
        "                reward_dict_value_value = list(reward_dict_value[0].values())\n",
        "                if type(reward_dict_value_value[0]) is dict:\n",
        "                    self.type = 'state_action_nextstate_dict'\n",
        "                else:\n",
        "                    self.type = 'state_action_dict'\n",
        "            else:\n",
        "                self.type = 'state_dict'\n",
        "        else:\n",
        "            self.reward_dict = {}\n",
        "            self.type = 'state_dict'\n",
        "#        print('self.type:',self.type)\n",
        "\n",
        "        if self.type == 'state_dict':\n",
        "            for ts in terminal_states:\n",
        "                self.reward_dict[ts] = terminal_state_reward\n",
        "\n",
        "        self.step_cost = step_cost\n",
        "\n",
        "        #set rmax\n",
        "        if rmax is None:\n",
        "            if self.type == 'state_dict':\n",
        "                rs = list(self.reward_dict.values()) + [default_reward,]\n",
        "                rmax = max(rs)\n",
        "            elif self.type == 'state_action_dict':\n",
        "                rmax = -np.inf\n",
        "                for s, ar in self.reward_dict.items():\n",
        "                    for a, r in ar.items():\n",
        "                        rmax = max(rmax, r)\n",
        "            elif self.type == 'state_feature_based':\n",
        "                fr = np.array(self.feature_rewards.values())\n",
        "                pos_fr = fr[fr>0]\n",
        "                if (len(pos_fr) == 0):\n",
        "                    pos_fr = [max(fr),]\n",
        "                rmax = np.sum(pos_fr)\n",
        "            else:\n",
        "                raise ValueError(\"Cannot set Rmax\")\n",
        "        self.rmax = rmax\n",
        "\n",
        "        self.reward_cache = {}\n",
        "        self.cache_rewards = cache_rewards\n",
        "\n",
        "    def reward(self, s=None, a=None, ns=None):\n",
        "\n",
        "        if self.type == 'state_dict':\n",
        "            reward = self.reward_dict.get(ns, self.default_reward)\n",
        "        elif self.type == \"state_action_dict\":\n",
        "            if s not in self.reward_dict:\n",
        "                reward = self.default_reward\n",
        "            else:\n",
        "                reward = self.reward_dict[s].get(a, self.default_reward)\n",
        "        elif self.type == \"state_action_nextstate_dict\":\n",
        "            if s not in self.reward_dict:\n",
        "                reward = self.default_reward\n",
        "            elif a not in self.reward_dict[s]:\n",
        "                reward = self.default_reward\n",
        "            else:\n",
        "                reward = self.reward_dict[s][a].get(ns, self.default_reward)\n",
        "        elif self.type == 'state_feature_based':\n",
        "            if ns in self.terminal_states:\n",
        "                reward = self.terminal_state_reward\n",
        "            elif self.cache_rewards:\n",
        "                if ns not in self.reward_cache:\n",
        "                    fs = self.state_features.get(ns, [])\n",
        "                    r = 0\n",
        "                    for f in fs:\n",
        "                        r += self.feature_rewards.get(f, self.default_reward)\n",
        "                    self.reward_cache[ns] = r\n",
        "                reward = self.reward_cache[ns]\n",
        "            else:\n",
        "                fs = self.state_features.get(ns, [])\n",
        "                reward = np.sum([self.feature_rewards[f] for f in fs])\n",
        "        elif self.type == 'state_action_feature_based':\n",
        "            pass\n",
        "\n",
        "        elif self.type == 'state_action_nextstate_feature_based':\n",
        "            pass\n",
        "\n",
        "        if ns in self.terminal_states:\n",
        "            return reward\n",
        "        else:\n",
        "            return reward + self.step_cost\n",
        "\n",
        "    def gen_reward_dict(self, states=None, state_actions=None,\n",
        "                        state_action_nextstates=None, tf=None,\n",
        "                        include_actions=False, include_nextstates=False):\n",
        "        # ================================================ #\n",
        "        #  Generate a state-action-nextstate rf dictionary #\n",
        "        # ================================================ #\n",
        "        if (include_actions and include_nextstates) \\\n",
        "                or self.type in ['state_action_nextstate_dict',\\\n",
        "                                 'state_action_nextstate_feature_based']:\n",
        "            rf ={}\n",
        "            for s, a_ns in state_action_nextstates.iteritems():\n",
        "                rf[s] = {}\n",
        "                for a, nstates in a_ns.items():\n",
        "                    rf[s][a] = {}\n",
        "                    for ns in nstates:\n",
        "                        #=========================================#\n",
        "                        #      Handle the different rf types      #\n",
        "                        #=========================================#\n",
        "                        if self.type in ['state_dict', 'state_feature_based']:\n",
        "                            rf[s][a][ns] = self.reward(ns=ns)\n",
        "\n",
        "                        elif self.type in ['state_action_dict',\n",
        "                                           'state_action_feature_based']:\n",
        "                            rf[s][a][ns] = self.reward(s=s, a=a)\n",
        "\n",
        "                        elif self.type in ['state_action_nextstate_dict',\n",
        "                                           'state_action_nextstate_feature_based']:\n",
        "                            rf[s][a][ns] = self.reward(s=s, a=a, ns=ns)\n",
        "\n",
        "                        else:\n",
        "                            raise ValueError(\"Undefined reward function dictionary!\")\n",
        "        # ================================================ #\n",
        "        #       Generate a state-action rf dictionary      #\n",
        "        # ================================================ #\n",
        "        elif include_actions or self.type in ['state_action_dict',\n",
        "                                              'state_action_feature_based']:\n",
        "            rf = {}\n",
        "            # ======================================== #\n",
        "            #      Handle the different rf types       #\n",
        "            # ======================================== #\n",
        "            if self.type in ['state_dict', 'state_feature_based']:\n",
        "                for s, a_ns in state_action_nextstates.items():\n",
        "                    rf[s] = {}\n",
        "                    for a, nstates in a_ns.items():\n",
        "                        if len(nstates) > 1:\n",
        "                            raise ValueError(\"Undefinable reward function dictionary!\")\n",
        "                        rf[s][a] = self.reward(ns=nstates[0])\n",
        "            elif self.type in ['state_action_dict','state_action_feature_based']:\n",
        "                for s, actions in state_actions.items():\n",
        "                    rf[s] = {}\n",
        "                    for a in actions:\n",
        "                        rf[s][a] = self.reward(s=s, a=a)\n",
        "            else:\n",
        "                raise ValueError(\"Undefined reward function dictionary!\")\n",
        "\n",
        "        # ================================================ #\n",
        "        #           Generate a state rf dictionary         #\n",
        "        # ================================================ #\n",
        "        elif self.type in ['state_dict', 'state_feature_based']:\n",
        "            rf = {ns: self.reward(ns=ns) for ns in states}\n",
        "        else:\n",
        "            raise ValueError(\"Undefined reward function dictionary!\")\n",
        "        return rf\n",
        "\n",
        "    def __hash__(self):\n",
        "        try:\n",
        "            return self.hash\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        #todo write a test for this hash function\n",
        "        myhash = [self.type,\n",
        "                  self.terminal_state_reward,\n",
        "                  self.terminal_states,\n",
        "                  self.default_reward,\n",
        "                  self.step_cost]\n",
        "\n",
        "        if self.type == 'state_dict':\n",
        "            myhash.extend([\n",
        "                tuple(sorted(self.reward_dict.items())),\n",
        "            ])\n",
        "        else:\n",
        "            myhash.extend([\n",
        "                False,\n",
        "            ])\n",
        "\n",
        "        if self.type == 'state_action_dict':\n",
        "            sar = []\n",
        "            for s, ar in self.reward_dict.items():\n",
        "                ar = tuple(sorted(ar.items()))\n",
        "                sar.append((s, ar))\n",
        "            sar = tuple(sorted(sar))\n",
        "            myhash.extend([sar,])\n",
        "        else:\n",
        "            myhash.extend([False,])\n",
        "\n",
        "        if self.type == 'state_action_nextstate_dict':\n",
        "            sansr = []\n",
        "            for s, ansr in self.reward_dict.items():\n",
        "                ansr_ = []\n",
        "                for a, nsr in ansr.iteritems():\n",
        "                    nsr = tuple(sorted(nsr.items()))\n",
        "                    ansr_.append((a, nsr))\n",
        "                ansr_ = tuple(sorted(ansr_))\n",
        "                sansr.append(ansr_)\n",
        "            sansr = tuple(sorted(sansr))\n",
        "            myhash.extend([sansr,])\n",
        "        else:\n",
        "            myhash.extend([False,])\n",
        "\n",
        "        if self.type == 'state_feature_based':\n",
        "            myhash.extend([\n",
        "                tuple(sorted(self.state_features.items())),\n",
        "                tuple(sorted(self.feature_rewards.items()))\n",
        "            ])\n",
        "        else:\n",
        "            myhash.extend([False,\n",
        "                           False])\n",
        "\n",
        "        self.hash = hash(tuple(myhash))\n",
        "\n",
        "        return hash(tuple(myhash))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if isinstance(other, self.__class__):\n",
        "            return hash(self) == hash(other)\n",
        "        return False\n",
        "\n",
        "\n",
        "class MDP(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def transition(self, s, a):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def reward(self, s=None, a=None, ns=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def available_actions(self, s):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def solve(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def state_hasher(self, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def state_unhasher(self, hashed):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_optimal_policy(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_softmax_function(self, temp):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_init_state(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def gen_transition_dict(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def calc_trajectory_return(self, traj, init_state=None, discount=1):\n",
        "        value = 0\n",
        "\n",
        "        if len(traj[0]) == 1:\n",
        "            ns = init_state\n",
        "\n",
        "        for tup in traj:\n",
        "            if len(tup) == 3:\n",
        "                s, a, ns = tup\n",
        "            elif len(tup) == 2:\n",
        "                s, a = tup\n",
        "                ns = self.transition(s, a)\n",
        "            elif len(tup) == 1:\n",
        "                a = tup\n",
        "                s = ns\n",
        "                ns = self.transition(s, a)\n",
        "            value += self.reward(s=s, a=a, ns=ns)*discount\n",
        "        return value\n",
        "\n",
        "    def build_transition_graph(self, init_state, max_nodes=np.inf):\n",
        "        graph = {}\n",
        "        frontier = [init_state, ]\n",
        "\n",
        "        while len(graph) < max_nodes:\n",
        "            s = frontier.pop()\n",
        "\n",
        "    def run(self, policy=None, init_state=None, max_steps=25, temp=1):\n",
        "        traj = []\n",
        "        if init_state is None:\n",
        "            init_state = self.get_init_state()\n",
        "        if policy is None:\n",
        "            policy = self.get_softmax_function(temp)\n",
        "        s = init_state\n",
        "        i = 0\n",
        "        while i < max_steps:\n",
        "            a = sample_prob_dict(policy[s])\n",
        "            ns = self.transition(s, a)\n",
        "            r = self.reward(s, a, ns)\n",
        "            traj.append((s, a, ns, r))\n",
        "            s = ns\n",
        "            if self.is_terminal(s):\n",
        "                break\n",
        "        return traj\n",
        "\n",
        "\n",
        "class GridWorld(MDP):\n",
        "    \"\"\"\n",
        "      Class defining a GridWorld object, which is a special case\n",
        "      of more general MDPs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, width=None, height=None,\n",
        "                 gridworld_array=None,\n",
        "                 wait_action=False,\n",
        "                 wall_action=False,\n",
        "                 state_features=None,\n",
        "                 absorbing_states=None,\n",
        "                 slip_states=None,\n",
        "                 slip_features=None,\n",
        "                 sticky_states=None,\n",
        "                 non_std_t_states=None,\n",
        "                 non_std_t_features=None,\n",
        "                 walls=None, #[((x, y), side),...]\n",
        "                 starting_states=None,\n",
        "                 state_rewards=None, #deprecated\n",
        "                 reward_dict=None,\n",
        "                 default_reward=0,\n",
        "                 step_cost=0,\n",
        "                 feature_rewards=None,\n",
        "                 include_intermediate_terminal=False,\n",
        "                 init_state=None,\n",
        "                 state_types=None,\n",
        "                 feature_types=None,\n",
        "                 discount_rate=None):\n",
        "\n",
        "        if gridworld_array is not None:\n",
        "            w = len(gridworld_array[0])\n",
        "            h = len(gridworld_array)\n",
        "            state_features = {(x, y): gridworld_array[h - 1 - y][x] for x, y in\n",
        "                              product(range(w), range(h))}\n",
        "            width = w\n",
        "            height = h\n",
        "\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.states = list(product(range(width), range(height))) + [(-1,-1),(-2,-2)]\n",
        "        self.wait_action = wait_action\n",
        "        self.wall_action = wall_action\n",
        "        self.cached_transitions = {}\n",
        "        self.include_intermediate_terminal = include_intermediate_terminal\n",
        "        self.intermediate_terminal = (-2, -2)\n",
        "        self.terminal_state = (-1, -1)\n",
        "\n",
        "        if absorbing_states is None:\n",
        "            absorbing_states = []\n",
        "        absorbing_states = copy.deepcopy(absorbing_states)\n",
        "        self.absorbing_states = frozenset(absorbing_states)\n",
        "\n",
        "        if state_features is None:\n",
        "            state_features = {}\n",
        "        self.state_features = state_features\n",
        "\n",
        "        #non-standard transitions\n",
        "        non_std_t_moves = ['forward', 'back', 'left', 'right',\n",
        "                           '2forward', '2back', 'horseleft',\n",
        "                           'horseright', 'diagleft', 'diagright',\n",
        "                           'stay']\n",
        "        self.non_std_t_moves = non_std_t_moves\n",
        "        non_std_t_states = copy.deepcopy(non_std_t_states)\n",
        "\n",
        "        if non_std_t_states is None:\n",
        "            non_std_t_states = {}\n",
        "\n",
        "        if slip_states is not None:\n",
        "            non_std_t_states.update(slip_states)\n",
        "\n",
        "        if non_std_t_features is None:\n",
        "            non_std_t_features = {}\n",
        "        if slip_features is not None:\n",
        "            non_std_t_features.update(slip_features)\n",
        "\n",
        "        if state_types is None:\n",
        "            state_types = {}\n",
        "\n",
        "        if feature_types is None:\n",
        "            feature_types = {}\n",
        "\n",
        "        for s in self.states:\n",
        "            f = state_features.get(s, None)\n",
        "            if f in non_std_t_features:\n",
        "                non_std_t_states[s] = non_std_t_features[f]\n",
        "            if f in feature_types:\n",
        "                state_types[s] = state_types.get(s, [])\n",
        "                state_types[s].append(feature_types[f])\n",
        "\n",
        "        for s, non_std_t in non_std_t_states.items():\n",
        "            if 'side' in non_std_t:\n",
        "                non_std_t['left'] = non_std_t['side'] / 2\n",
        "                non_std_t['right'] = non_std_t['side'] / 2\n",
        "                del non_std_t['side']\n",
        "            if 'horsemove' in non_std_t:\n",
        "                non_std_t['horseleft'] = non_std_t['horsemove'] / 2\n",
        "                non_std_t['horseright'] = non_std_t['horsemove'] / 2\n",
        "                del non_std_t['horsemove']\n",
        "            if 'diag' in non_std_t:\n",
        "                non_std_t['diagleft'] = non_std_t['diag'] / 2\n",
        "                non_std_t['diagright'] = non_std_t['diag'] / 2\n",
        "                del non_std_t['diag']\n",
        "            non_std_t = [non_std_t.get(move, 0) for move in non_std_t_moves]\n",
        "            non_std_t_states[s] = non_std_t\n",
        "        self.non_std_t_states = non_std_t_states\n",
        "        self.state_types = state_types\n",
        "\n",
        "        #walls\n",
        "        if walls is None:\n",
        "            walls = []\n",
        "        self.walls = walls\n",
        "\n",
        "        #initial states\n",
        "        if starting_states is None:\n",
        "            starting_states = []\n",
        "        starting_states = copy.deepcopy(starting_states)\n",
        "\n",
        "        if init_state is not None:\n",
        "            starting_states.append(init_state)\n",
        "        self.starting_states = frozenset(starting_states)\n",
        "\n",
        "        # reward function stuff\n",
        "        self.reward_function = RewardFunction(state_features=state_features,\n",
        "                                              state_rewards=state_rewards,\n",
        "                                              feature_rewards=feature_rewards,\n",
        "                                              reward_dict=reward_dict,\n",
        "                                              default_reward=default_reward,\n",
        "                                              step_cost=step_cost)\n",
        "        self.rmax = self.reward_function.rmax\n",
        "        self.terminal_state_reward = self.reward_function.terminal_state_reward\n",
        "\n",
        "        n_actions = 4\n",
        "\n",
        "        if self.wait_action:\n",
        "            n_actions += 1\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.reward_cache = {}\n",
        "        self.transition_cache = {}\n",
        "        self.available_action_cache = {}\n",
        "        self.discount_rate = discount_rate\n",
        "\n",
        "    def __hash__(self):\n",
        "        try:\n",
        "            return self.hash\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        self.hash = hash((\n",
        "            self.width,\n",
        "            self.height,\n",
        "            self.wait_action,\n",
        "            self.wall_action,\n",
        "            self.reward_function,\n",
        "            self.absorbing_states,\n",
        "            frozenset([(s, nst) for s, nst in self.non_std_t_states.iteritems()]),\n",
        "            self.starting_states,\n",
        "            self.include_intermediate_terminal,\n",
        "            self.intermediate_terminal,\n",
        "            self.terminal_state\n",
        "        ))\n",
        "        return self.hash\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if isinstance(other, self.__class__):\n",
        "            return hash(self) == hash(other)\n",
        "        return False\n",
        "\n",
        "    # ============================================== #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    #          State/Action Related Methods          #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    # ============================================== #\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        return s == self.terminal_state\n",
        "\n",
        "    def is_any_terminal(self, s):\n",
        "        return s in [self.terminal_state, self.intermediate_terminal]\n",
        "\n",
        "    def is_absorbing(self, s):\n",
        "        return s in self.absorbing_states\n",
        "\n",
        "    def state_hasher(self, state):\n",
        "        return state\n",
        "\n",
        "    def state_unhasher(self, hashed):\n",
        "        return hashed\n",
        "\n",
        "    def get_init_state(self):\n",
        "        if len(self.starting_states) == 0:\n",
        "            raise ValueError('No initial state defined')\n",
        "        starting_states = list(self.starting_states)\n",
        "        i = np.random.choice(len(starting_states))\n",
        "        return starting_states[i]\n",
        "\n",
        "    def available_actions(self, s):\n",
        "        try:\n",
        "            return self.available_action_cache[s]\n",
        "        except:\n",
        "            pass\n",
        "        actions = []\n",
        "\n",
        "        # handle absorbing, terminal, and intermediate terminal states\n",
        "        if s in self.absorbing_states:\n",
        "            actions.append('%')\n",
        "        elif s == self.terminal_state:\n",
        "            actions.append('%')\n",
        "        elif s == self.intermediate_terminal:\n",
        "            actions.append('%')\n",
        "\n",
        "#         handle 'normal' transitions with no wall actions\n",
        "        elif not self.wait_action:\n",
        "            if s[1] < self.height - 1 and (s, '^') not in self.walls:\n",
        "                actions.append('^')\n",
        "            if s[1] > 0 and (s, 'v') not in self.walls:\n",
        "                actions.append('v')\n",
        "            if s[0] < self.width - 1 and (s, '>') not in self.walls:\n",
        "                actions.append('>')\n",
        "            if s[0] > 0 and (s, '<') not in self.walls:\n",
        "                actions.append('<')\n",
        "        elif self.wall_action:\n",
        "            actions.extend(['^','v','<','>'])\n",
        "\n",
        "        if self.wait_action:\n",
        "            actions.append('x')\n",
        "\n",
        "        self.available_action_cache[s] = actions\n",
        "\n",
        "        return actions\n",
        "\n",
        "    # ============================================== #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    #          Transition Function Methods           #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    # ============================================== #\n",
        "\n",
        "    def _normal_transition(self, s, a):\n",
        "        \"\"\"\n",
        "        \"normal\" transitions (taking into account walls)\n",
        "        \"\"\"\n",
        "\n",
        "        # handle walls\n",
        "        if (s, a) in self.walls:\n",
        "            res = s\n",
        "\n",
        "        #handle non-wall transitions\n",
        "        elif s[1] < self.height - 1 and a == '^':\n",
        "            res = s[0], s[1] + 1\n",
        "        elif s[1] > 0 and a == 'v':\n",
        "            res = s[0], s[1] - 1\n",
        "        elif s[0] < self.width - 1 and a == '>':\n",
        "            res = s[0] + 1, s[1]\n",
        "        elif s[0] > 0 and a == '<':\n",
        "            res = s[0] - 1, s[1]\n",
        "        elif a == 'x':\n",
        "            res = s\n",
        "\n",
        "        #handle default transition\n",
        "        else:\n",
        "            res = s\n",
        "        return res\n",
        "\n",
        "    def _get_side_actions(self, a):\n",
        "        if a in '^v':\n",
        "            return ['<', '>']\n",
        "        elif a in '<>':\n",
        "            return ['^', 'v']\n",
        "        else:\n",
        "            return a\n",
        "\n",
        "    def _get_back_action(self, a):\n",
        "        if a == '^':\n",
        "            a_ = 'v'\n",
        "        elif a == 'v':\n",
        "            a_ = '^'\n",
        "        elif a == '<':\n",
        "            a_ = '>'\n",
        "        elif a == '>':\n",
        "            a_ = '<'\n",
        "        else:\n",
        "            a_ = a\n",
        "        return a_\n",
        "\n",
        "    def _get_right_action(self, a):\n",
        "        if a == '>':\n",
        "            return 'v'\n",
        "        elif a == 'v':\n",
        "            return '<'\n",
        "        elif a == '<':\n",
        "            return '^'\n",
        "        elif a == '^':\n",
        "            return '>'\n",
        "        else:\n",
        "            return a\n",
        "\n",
        "    def _get_left_action(self, a):\n",
        "        if a == '>':\n",
        "            return '^'\n",
        "        elif a == 'v':\n",
        "            return '>'\n",
        "        elif a == '<':\n",
        "            return 'v'\n",
        "        elif a == '^':\n",
        "            return '<'\n",
        "        else:\n",
        "            return a\n",
        "\n",
        "    def transition_dist(self, s, a):\n",
        "        try:\n",
        "            return self.transition_cache[(s, a)]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        dist = {}\n",
        "\n",
        "        # handle absorbing, terminal, and intermediate terminal states\n",
        "        if s in self.absorbing_states:\n",
        "            if self.include_intermediate_terminal:\n",
        "                dist = {self.intermediate_terminal : 1}\n",
        "            else:\n",
        "                dist = {self.terminal_state : 1}\n",
        "\n",
        "        elif s == self.terminal_state:\n",
        "            dist = {self.terminal_state : 1}\n",
        "\n",
        "        elif s == self.intermediate_terminal:\n",
        "            dist = {self.terminal_state : 1}\n",
        "\n",
        "        #non-standard transition states\n",
        "        elif s in self.non_std_t_states:\n",
        "            nst = self.non_std_t_states[s]\n",
        "\n",
        "            for mi, move in enumerate(self.non_std_t_moves):\n",
        "                p = nst[mi]\n",
        "                if p == 0:\n",
        "                    continue\n",
        "\n",
        "                if move == 'forward':\n",
        "                    res = self._normal_transition(s, a)\n",
        "                elif move == 'back':\n",
        "                    a_ = self._get_back_action(a)\n",
        "                    res = self._normal_transition(s, a_)\n",
        "                elif move == 'left':\n",
        "                    a_ = self._get_left_action(a)\n",
        "                    res = self._normal_transition(s, a_)\n",
        "                elif move == 'right':\n",
        "                    a_ = self._get_right_action(a)\n",
        "                    res = self._normal_transition(s, a_)\n",
        "                elif move == '2forward':\n",
        "                    ns = self._normal_transition(s, a)\n",
        "                    res = self._normal_transition(ns, a)\n",
        "                elif move == '2back':\n",
        "                    a_ = self._get_back_action(a)\n",
        "                    ns = self._normal_transition(s, a_)\n",
        "                    res = self._normal_transition(ns, a_)\n",
        "                elif move == 'horseleft':\n",
        "                    ns = self._normal_transition(s, a)\n",
        "                    ns = self._normal_transition(ns, a)\n",
        "                    a_ = self._get_left_action(a)\n",
        "                    res = self._normal_transition(ns, a_)\n",
        "                elif move == 'horseright':\n",
        "                    ns = self._normal_transition(s, a)\n",
        "                    ns = self._normal_transition(ns, a)\n",
        "                    a_ = self._get_right_action(a)\n",
        "                    res = self._normal_transition(ns, a_)\n",
        "                elif move == 'diagleft':\n",
        "                    ns = self._normal_transition(s, a)\n",
        "                    a_ = self._get_left_action(a)\n",
        "                    res = self._normal_transition(ns, a_)\n",
        "                elif move == 'diagright':\n",
        "                    ns = self._normal_transition(s, a)\n",
        "                    a_ = self._get_right_action(a)\n",
        "                    res = self._normal_transition(ns, a_)\n",
        "                elif move == 'stay':\n",
        "                    res = s\n",
        "                dist[res] = dist.get(res, 0)\n",
        "                dist[res] += p\n",
        "\n",
        "        #\"normal\" transitions (taking into account walls)\n",
        "        else:\n",
        "            ns = self._normal_transition(s, a)\n",
        "            dist = {ns: 1}\n",
        "\n",
        "        self.transition_cache[(s, a)] = dist\n",
        "        return dist\n",
        "\n",
        "    def transition(self, s, a):\n",
        "        dist = self.transition_dist(s, a)\n",
        "        ns, p = zip(*list(dist.items()))\n",
        "        return ns[np.random.choice(len(ns), p=p)]\n",
        "\n",
        "    def gen_transition_dict(self):\n",
        "        tf = {}\n",
        "        for s in self.states:\n",
        "            tf[s] = {}\n",
        "            for a in self.available_actions(s):\n",
        "                tf[s][a] = self.transition_dist(s, a)\n",
        "        return tf\n",
        "\n",
        "    def expected_transition(self, s, a):\n",
        "        \"\"\"Transition assuming no walls or whatever\"\"\"\n",
        "        if a == '^':\n",
        "            res = s[0], s[1]+1\n",
        "        elif a == 'v':\n",
        "            res = s[0], s[1] - 1\n",
        "        elif a == '>':\n",
        "            res = s[0] + 1, s[1]\n",
        "        elif a == '<':\n",
        "            res = s[0] - 1, s[1]\n",
        "        else:\n",
        "            res = s\n",
        "        return res\n",
        "\n",
        "    # ============================================== #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    #             Reward Function Methods            #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    # ============================================== #\n",
        "\n",
        "    def reward(self, s=None, a=None, ns=None):\n",
        "        try:\n",
        "            return self.reward_cache[(s, a, ns)]\n",
        "        except KeyError:\n",
        "            pass\n",
        "        res = self.reward_function.reward(s=s, a=a, ns=ns)\n",
        "        self.reward_cache[(s, a, ns)] = res\n",
        "        return res\n",
        "\n",
        "    def gen_reward_dict(self, include_actions=False, include_nextstates=False):\n",
        "        state_actions = {s: self.available_actions(s) for s in self.states}\n",
        "#        print(state_actions)\n",
        "        state_action_nextstates = {}\n",
        "        for s in self.states:\n",
        "            state_action_nextstates[s] = {}\n",
        "            for a in self.available_actions(s):\n",
        "#                print(self.transition_dist(s, a), type(self.transition_dist(s,a).keys()))\n",
        "                state_action_nextstates[s][a] = self.transition_dist(s, a).keys()\n",
        "#                              copy.deepcopy(self.transition_dist(s, a).keys())\n",
        "        rf = self.reward_function.gen_reward_dict(\n",
        "                states=self.states,\n",
        "                state_actions=state_actions,\n",
        "                state_action_nextstates=state_action_nextstates,\n",
        "                tf={},\n",
        "                include_actions=include_actions,\n",
        "                include_nextstates=include_nextstates)\n",
        "\n",
        "\n",
        "        return rf\n",
        "\n",
        "    # ============================================== #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    #              Computation Methods               #\n",
        "    #                                                #\n",
        "    #                                                #\n",
        "    # ============================================== #\n",
        "\n",
        "    def solve(self, start_state=None, **kwargs):\n",
        "        if 'gamma' not in kwargs and self.discount_rate is not None:\n",
        "            kwargs['gamma'] = self.discount_rate\n",
        "        rf = self.gen_reward_dict()\n",
        "        tf = self.gen_transition_dict()\n",
        "        op, vf, av = deterministicVI(rf, tf, init_state=start_state, **kwargs)\n",
        "        self.optimal_policy = op\n",
        "        self.value_function = vf\n",
        "        self.action_value_function = av\n",
        "\n",
        "        self.solved = True\n",
        "        return\n",
        "\n",
        "    def get_optimal_policy(self):\n",
        "        if not self.solved:\n",
        "            raise ValueError(\"No optimal policy computed\")\n",
        "        return self.optimal_policy\n",
        "\n",
        "    def get_softmax_function(self, temp=1):\n",
        "        return calc_softmax_policy(self.action_value_function, temp)\n",
        "\n",
        "# ================================#\n",
        "#\n",
        "#\tVisualization stuff\n",
        "#\n",
        "# ================================#\n",
        "\n",
        "def visualize_states(ax=None, states=None,\n",
        "                     tile_color=None,\n",
        "                     plot_size=None,\n",
        "                     panels=None,\n",
        "                     **kwargs):\n",
        "    '''\n",
        "        Supported kwargs:\n",
        "            - tile_color : a dictionary from tiles (states) to colors\n",
        "            - plot_size is an integer specifying how many tiles wide\n",
        "              and high the plot is, with the grid itself in the middle\n",
        "    '''\n",
        "    if tile_color is None:\n",
        "        tile_color = {}\n",
        "\n",
        "    if ax == None:\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "\n",
        "    if panels is None:\n",
        "        panels = []\n",
        "\n",
        "    # plot squares\n",
        "    for s in states:\n",
        "        if s == (-1, -1):\n",
        "            continue\n",
        "        square = Rectangle(xy=s, width=1, height=1, color=tile_color.get(s, 'white'),\\\n",
        "                           ec='k', lw=2)\n",
        "        ax.add_patch(square)\n",
        "\n",
        "    ax.axis('off')\n",
        "    if plot_size is None and len(panels) == 0:\n",
        "        ax.set_xlim(-0.1, 1 + max([s[0] for s in states]) + .1)\n",
        "        ax.set_ylim(-0.1, 1 + max([s[1] for s in states]) + .1)\n",
        "        ax.axis('scaled')\n",
        "    elif len(panels) > 0:\n",
        "        xlim = [-0.1, 1 + max([s[0] for s in states]) + .1]\n",
        "        ylim = [-0.1, 1 + max([s[1] for s in states]) + .1]\n",
        "        if 'right' in panels:\n",
        "            xlim[1] += 2\n",
        "        if 'left' in panels:\n",
        "            xlim[0] -= 2\n",
        "        if 'top' in panels:\n",
        "            ylim[1] += 2\n",
        "        if 'bottom' in panels:\n",
        "            ylim[0] -= 2\n",
        "        ax.set_xlim(*xlim)\n",
        "        ax.set_ylim(*ylim)\n",
        "    else:\n",
        "        cx = (max([s[0] for s in states]) + 1) / 2\n",
        "        cy = (max([s[1] for s in states]) + 1) / 2\n",
        "        ax.set_xlim(cx - 0.1 - plot_size / 2, cx + 0.1 + plot_size / 2)\n",
        "        ax.set_ylim(cy - 0.1 - plot_size / 2, cy + 0.1 + plot_size / 2)\n",
        "    return ax\n",
        "\n",
        "def visualize_deterministic_policy(ax, policy, absorbing_action='%', **kwargs):\n",
        "    m = [(s, {a: 1.0}) for s, a in policy.items() if a != absorbing_action]\n",
        "    policy = dict(m)\n",
        "    return visualize_action_values(ax, policy, **kwargs)\n",
        "\n",
        "def visualize_action_values(ax=None, state_action_values=None,\n",
        "                            color_valence=False,\n",
        "                            global_maxval=None, **kwargs):\n",
        "    '''\n",
        "        Supported kwargs:\n",
        "            - color_valence : boolean whether to color negative red and positive blue, otherwise color is always black\n",
        "            - global_maxval : max value to normalize arrow lengths to\n",
        "    '''\n",
        "\n",
        "    # plot arrows\n",
        "    if global_maxval is None:\n",
        "        global_maxval = -np.inf\n",
        "        for s, a_v in state_action_values.items():\n",
        "            for v in a_v.values():\n",
        "                if global_maxval < np.absolute(v):\n",
        "                    global_maxval = np.absolute(v)\n",
        "\n",
        "    for s, a_v in state_action_values.items():\n",
        "        if s == (-1, -1):\n",
        "            continue\n",
        "        x, y = s\n",
        "#        normalization = np.sum(np.absolute(a_v.values()))\n",
        "#        maxval = max(np.absolute(a_v.values()))\n",
        "        for a, v in a_v.items():\n",
        "            if a == '%' or v == 0:\n",
        "                continue\n",
        "\n",
        "            mag = (.5 / global_maxval) * np.absolute(v)\n",
        "\n",
        "            if color_valence:\n",
        "                if v <= 0:\n",
        "                    arrowColor = 'red'\n",
        "                else:\n",
        "                    arrowColor = 'blue'\n",
        "            else:\n",
        "                arrowColor = 'k'\n",
        "            arrowwidth = .1\n",
        "            if a == '<':\n",
        "                ax.add_patch(Arrow(x + .5, y + .5, -mag, 0, width=arrowwidth,\n",
        "                                   color=arrowColor))\n",
        "            elif a == '>':\n",
        "                ax.add_patch(Arrow(x + .5, y + .5, mag, 0, width=arrowwidth,\n",
        "                                   color=arrowColor))\n",
        "            elif a == 'v':\n",
        "                ax.add_patch(Arrow(x + .5, y + .5, 0, -mag, width=arrowwidth,\n",
        "                                   color=arrowColor))\n",
        "            elif a == '^':\n",
        "                ax.add_patch(Arrow(x + .5, y + .5, 0, mag, width=arrowwidth,\n",
        "                                   color=arrowColor))\n",
        "            elif a == 'x':\n",
        "                ax.add_patch(\n",
        "                    Circle((x + .5, y + .5), radius=mag * .9, fill=False))\n",
        "            else:\n",
        "                raise Exception('unknown action')\n",
        "    return ax\n",
        "\n",
        "def visualize_trajectory(axis, traj,\n",
        "                         jitter_mean=0,\n",
        "                         jitter_var=.1,\n",
        "                         plot_actions=False,\n",
        "                         endpoint_jitter=False,\n",
        "                         color='black',\n",
        "                         **kwargs):\n",
        "\n",
        "    traj = [(t[0], t[1]) for t in traj]  # traj only depends on state actions\n",
        "\n",
        "    if len(traj) == 2:\n",
        "        p0 = tuple(np.array(traj[0][0]) + .5)\n",
        "        p2 = tuple(np.array(traj[1][0]) + .5)\n",
        "        p1 = np.array([(p0[0] + p2[0]) / 2, (p0[1] + p2[1]) / 2]) \\\n",
        "             + np.random.normal(0, jitter_var, 2)\n",
        "        if endpoint_jitter:\n",
        "            p0 = tuple(\n",
        "                np.array(p0) + np.random.normal(jitter_mean, jitter_var, 2))\n",
        "            p1 = tuple(\n",
        "                np.array(p1) + np.random.normal(jitter_mean, jitter_var, 2))\n",
        "        segments = [[p0, p1, p2], ]\n",
        "    elif (len(traj) == 3) and (traj[0][0] == traj[2][0]):\n",
        "        p0 = tuple(np.array(traj[0][0]) + .5)\n",
        "        p2 = tuple(np.array(traj[1][0]) + .5)\n",
        "        if abs(p0[0] - p2[0]) > 0:  # horizontal\n",
        "            jitter = np.array(\n",
        "                [0, np.random.normal(jitter_mean, jitter_var * 2)])\n",
        "            p2 = p2 - np.array([.25, 0])\n",
        "        else:  # vertical\n",
        "            jitter = np.array(\n",
        "                [np.random.normal(jitter_mean, jitter_var * 2), 0])\n",
        "            p2 = p2 - np.array([0, .25])\n",
        "        p1 = p2 + jitter\n",
        "        p3 = p2 - jitter\n",
        "        segments = [[p0, p1, p2], [p2, p3, p0]]\n",
        "    else:\n",
        "        state_coords = []\n",
        "        for s, a in traj:\n",
        "            jitter = np.random.normal(jitter_mean, jitter_var, 2)\n",
        "            coord = np.array(s) + .5 + jitter\n",
        "            state_coords.append(tuple(coord))\n",
        "        if not endpoint_jitter:\n",
        "            state_coords[0] = tuple(np.array(traj[0][0]) + .5)\n",
        "            state_coords[-1] = tuple(np.array(traj[-1][0]) + .5)\n",
        "        join_point = state_coords[0]\n",
        "        segments = []\n",
        "        for i, s in enumerate(state_coords[:-1]):\n",
        "            ns = state_coords[i + 1]\n",
        "\n",
        "            segment = []\n",
        "            segment.append(join_point)\n",
        "            segment.append(s)\n",
        "            if i < len(traj) - 2:\n",
        "                join_point = tuple(np.mean([s, ns], axis=0))\n",
        "                segment.append(join_point)\n",
        "            else:\n",
        "                segment.append(ns)\n",
        "            segments.append(segment)\n",
        "\n",
        "    for segment, step in zip(segments, traj[:-1]):\n",
        "        state = step[0]\n",
        "        action = step[1]\n",
        "\n",
        "        codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3]\n",
        "        path = Path(segment, codes)\n",
        "        patch = patches.PathPatch(path, facecolor='none', capstyle='butt',\n",
        "                                  edgecolor=color, **kwargs)\n",
        "        axis.add_patch(patch)\n",
        "        if plot_actions:\n",
        "            dx = 0\n",
        "            dy = 0\n",
        "            if action == '>':\n",
        "                dx = 1\n",
        "            elif action == 'v':\n",
        "                dy = -1\n",
        "            elif action == '^':\n",
        "                dy = 1\n",
        "            elif action == '<':\n",
        "                dx = -1\n",
        "            action_arrow = patches.Arrow(segment[1][0], segment[1][1],\n",
        "                                         dx * .4,\n",
        "                                         dy * .4,\n",
        "                                         width=.25,\n",
        "                                         color='grey')\n",
        "            axis.add_patch(action_arrow)\n",
        "\n",
        "\n",
        "def plot_text(axis, state, text, outline=False, outline_linewidth=1,\n",
        "              outline_color='black',\n",
        "              x_offset=0, y_offset=0, **kwargs):\n",
        "    mytext = axis.text(state[0] + .5 + x_offset, state[1] + .5 + y_offset,\n",
        "                       text, **kwargs)\n",
        "    if outline:\n",
        "        mytext.set_path_effects([path_effects.Stroke(\n",
        "            linewidth=outline_linewidth, foreground=outline_color),\n",
        "            path_effects.Normal()])\n",
        "\n",
        "POS_FB = 10\n",
        "NEG_FB = -10\n",
        "NO_FB = 0\n",
        "GOAL_FB  = 15\n",
        "discount_rate = .95\n",
        "state_action_reward_function = True\n",
        "\n",
        "rm_fb = {(0,0): {'>': NEG_FB, '^': NO_FB},\n",
        "         (0,1): {'v': NO_FB, '>': NEG_FB, '^': NO_FB},\n",
        "         (0,2): {'v': NO_FB, '>': NO_FB},\n",
        "         (1,0): {'<': NO_FB, '>': NEG_FB, '^': NEG_FB},\n",
        "         (2,0): {'<': NEG_FB, '^': NEG_FB},\n",
        "         (1,1): {'v': NEG_FB, '<': NO_FB, '>': NEG_FB, '^': NO_FB},\n",
        "         (1,2): {'<': NO_FB, 'v': NEG_FB, '>': GOAL_FB},\n",
        "         (2,1): {'<': NEG_FB, 'v': NEG_FB, '^': GOAL_FB}}\n",
        "\n",
        "af_fb = {(0,0): {'>': NEG_FB, '^':POS_FB},\n",
        "         (0,1): {'v': NEG_FB, '^': POS_FB, '>': NEG_FB},\n",
        "         (0,2): {'v': NEG_FB, '>': POS_FB},\n",
        "         (1,0): {'<': POS_FB, '^': NEG_FB, '>':NEG_FB},\n",
        "         (1,1): {'<': NEG_FB, '^': POS_FB, 'v': NEG_FB, '>': NEG_FB},\n",
        "         (1,2): {'<': NEG_FB, 'v': NEG_FB, '>': GOAL_FB},\n",
        "         (2,0): {'<': NEG_FB, '^':POS_FB},\n",
        "         (2,1): {'<': NEG_FB, 'v': NEG_FB, '^': GOAL_FB}}\n",
        "\n",
        "##TODO: Change reward function here\n",
        "params = {\n",
        "        'gridworld_array': ['..g', '.xx', '.xx'],\n",
        "        'reward_dict': af_fb, #switch to af_fb for action feedback reward fn\n",
        "        'init_state': (0, 0),\n",
        "        'absorbing_states': [(2, 2), ]}\n",
        "\n",
        "gw = GridWorld(**params)\n",
        "\n",
        "gw.solve(gw.get_init_state(), gamma=discount_rate)\n",
        "print('='*100)\n",
        "\n",
        "myQlearning = Qlearning(mdp=gw, discount_rate=discount_rate)\n",
        "myQlearning.run()\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "f_colors = {\n",
        "        'z': 'grey',\n",
        "        'x': 'lightgrey',\n",
        "        'y': 'yellow',\n",
        "        '.': 'white',\n",
        "        'g': 'yellow'}\n",
        "\n",
        "t_colors = {s: f_colors[f] for s, f in gw.state_features.items()}\n",
        "\n",
        "tiles = list(product(range(gw.width), range(gw.height)))\n",
        "\n",
        "\n",
        "\n",
        "print(\"=======================Q-Learning Result=============================\")\n",
        "fig2 = plt.figure(figsize=(10, 8))\n",
        "# plot action value function\n",
        "ax = fig2.add_subplot(2, 2, 1)\n",
        "visualize_states(ax=ax, states=tiles, tile_color=t_colors)\n",
        "visualize_action_values(ax=ax,\n",
        "                        state_action_values=myQlearning.qvalues,\n",
        "                        color_valence=True)\n",
        "ax.set_title(\n",
        "    \"Q-Leaning Action-Value \\nFunction (discount = %.2f)\" % discount_rate)\n",
        "print(myQlearning.qvalues)\n",
        "\n",
        "# plot optimal deterministic policy\n",
        "#ax = fig2.add_subplot(2, 3, 3)\n",
        "#visualize_states(ax=ax, states=tiles, tile_color=t_colors)\n",
        "#visualize_deterministic_policy(ax, myQlearning.get_egreedy_policy())\n",
        "#ax.set_title(\n",
        "#    \"Q-learning \\nPolicy (discount = %.2f)\" % discount_rate)\n",
        "\n",
        "# plot softmax policy\n",
        "ax = fig2.add_subplot(2, 2, 2)\n",
        "visualize_states(ax=ax, states=tiles, tile_color=t_colors)\n",
        "softmaxtemp = 5\n",
        "softmax_policy = myQlearning.get_softmax_policy()\n",
        "#calc_softmax_policy(myQlearning.mdp.action_value_function,\n",
        "#                                     temp=softmaxtemp)\n",
        "visualize_action_values(ax, softmax_policy)\n",
        "ax.set_title(\n",
        "    \"Q-Softmax Policy\\n(discount = %.2f,\\ntemp = %.2f)\" %\n",
        "    (discount_rate, softmaxtemp))\n",
        "\n",
        "\n",
        "# plot trajectory\n",
        "ax = fig2.add_subplot(2, 2, 3)\n",
        "visualize_states(ax=ax, states=tiles, tile_color=t_colors)\n",
        "traj = []\n",
        "s = (0, 0)\n",
        "for _ in range(25):\n",
        "    a = myQlearning.get_action(s)\n",
        "    ns = myQlearning.mdp.transition(s, a)\n",
        "    traj.append((s, a, ns))\n",
        "    if myQlearning.mdp.is_terminal(ns):\n",
        "        break\n",
        "    s = ns\n",
        "visualize_trajectory(ax, traj, plot_actions=False)\n",
        "ax.set_title(\"Q-Learning \\nTrajectory Example 1\")\n",
        "\n",
        "ax = fig2.add_subplot(2, 2, 4)\n",
        "visualize_states(ax=ax, states=tiles, tile_color=t_colors)\n",
        "traj = []\n",
        "s = (1, 0)\n",
        "for _ in range(25):\n",
        "    a = myQlearning.get_action(s)\n",
        "    ns = myQlearning.mdp.transition(s, a)\n",
        "    traj.append((s, a, ns))\n",
        "    if myQlearning.mdp.is_terminal(ns):\n",
        "        break\n",
        "    s = ns\n",
        "visualize_trajectory(ax, traj, plot_actions=False)\n",
        "ax.set_title(\"Q-Learning \\nTrajectory Example 2\")\n"
      ]
    }
  ]
}